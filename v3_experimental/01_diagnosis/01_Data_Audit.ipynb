{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "908be995",
            "metadata": {},
            "source": [
                "# üî¨ Data Audit para Generaci√≥n Sint√©tica\n",
                "\n",
                "**Objetivo:** Diagnosticar estad√≠sticamente el dataset de entrenamiento y definir las reglas del juego para la generaci√≥n sint√©tica.\n",
                "\n",
                "**Autor:** Data Scientist Auditor  \n",
                "**Fecha:** 2026-01-08  \n",
                "**Versi√≥n:** 2.0 (Enhanced)\n",
                "\n",
                "---\n",
                "\n",
                "## üìö Marco Te√≥rico y Referencias Cient√≠ficas\n",
                "\n",
                "### Fundamentos de Datos de Supervivencia\n",
                "\n",
                "> **Lawless, J.F. (2003)**, *\"Statistical Models and Methods for Lifetime Data\"*, 2nd Ed., Wiley:\n",
                "> - Los datos de supervivencia tienen caracter√≠sticas especiales: tiempo positivo, censura a la derecha\n",
                "> - La tasa de censura afecta la potencia estad√≠stica y la validez de las estimaciones\n",
                "> - Para s√≠ntesis: preservar la distribuci√≥n marginal de duration y la correlaci√≥n con event\n",
                "\n",
                "### Tiempo-al-Empleo en Graduados\n",
                "\n",
                "> **Getie Ayaneh et al. (2020)**, DOI: [10.1155/2020/8653405](https://doi.org/10.1155/2020/8653405):\n",
                "> - \"Survival Models for the Analysis of Waiting Time to First Employment\"\n",
                "> - Censura t√≠pica: 40-60% en estudios de empleabilidad\n",
                "> - Variables clave: g√©nero, edad, tipo de carrera\n",
                "\n",
                "> **Alemu (2022)**, DOI: [10.1155/2022/2165610](https://doi.org/10.1155/2022/2165610):\n",
                "> - \"Understanding Waiting Time from Graduation to First Employment\"\n",
                "> - Recomienda an√°lisis de censura estratificado por cohorte\n",
                "\n",
                "### Datos Sint√©ticos para Survival Analysis\n",
                "\n",
                "> **Andonovikj et al. (2024)**, \"Survival analysis as semi-supervised multi-label classification\":\n",
                "> - Los sintetizadores deben preservar la relaci√≥n duration-event (no independientes)\n",
                "> - Restricci√≥n cr√≠tica: duration > 0 siempre\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a2d41292",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==============================================================================\n",
                "# CONFIGURACI√ìN\n",
                "# ==============================================================================\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import json\n",
                "from pathlib import Path\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Rutas relativas desde v3_experimental/01_diagnosis/\n",
                "DATA_PATH = Path(\"../../v2/data/processed/train_final.parquet\")\n",
                "OUTPUT_PATH = Path(\"dataset_diagnosis.json\")\n",
                "\n",
                "print(f\"‚úÖ Configuraci√≥n cargada\")\n",
                "print(f\"   Input: {DATA_PATH}\")\n",
                "print(f\"   Output: {OUTPUT_PATH}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fe6fe37f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==============================================================================\n",
                "# CARGAR DATOS (Solo TRAIN - NO TEST)\n",
                "# ==============================================================================\n",
                "\n",
                "df = pd.read_parquet(DATA_PATH)\n",
                "\n",
                "print(f\"üìä DATASET CARGADO:\")\n",
                "print(f\"   Filas: {len(df)}\")\n",
                "print(f\"   Columnas: {len(df.columns)}\")\n",
                "print(f\"\\n   Fuente: Encuesta reci√©n graduados - pregrado (EPN)\")\n",
                "print(f\"   Restricci√≥n: Solo datos de TRAIN para evitar data leakage\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section_classification",
            "metadata": {},
            "source": [
                "---\n",
                "## 1Ô∏è‚É£ Clasificaci√≥n de Columnas\n",
                "\n",
                "Seg√∫n Lawless (2003), es crucial distinguir:\n",
                "- **Continuas**: Variables con >10 valores √∫nicos (edad, duration)\n",
                "- **Discretas**: Variables con ‚â§10 valores √∫nicos\n",
                "- **Binarias**: Caso especial de discretas con exactamente 2 valores {0,1}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b24af25f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==============================================================================\n",
                "# CLASIFICACI√ìN DE COLUMNAS\n",
                "# ==============================================================================\n",
                "\n",
                "continuous_cols = []\n",
                "discrete_cols = []\n",
                "binary_cols = []\n",
                "categorical_cols = []\n",
                "zero_variance_cols = []  # NUEVO: Variables problem√°ticas\n",
                "column_info = {}\n",
                "\n",
                "for col in df.columns:\n",
                "    n_unique = df[col].nunique()\n",
                "    dtype = str(df[col].dtype)\n",
                "    \n",
                "    info = {\n",
                "        'dtype': dtype,\n",
                "        'n_unique': int(n_unique),\n",
                "        'null_count': int(df[col].isna().sum()),\n",
                "        'null_pct': float(df[col].isna().mean()),\n",
                "    }\n",
                "    \n",
                "    # Detectar varianza cero (NUEVO)\n",
                "    if n_unique <= 1:\n",
                "        zero_variance_cols.append(col)\n",
                "        info['category'] = 'zero_variance'\n",
                "        info['warning'] = 'No aporta informaci√≥n, considerar excluir'\n",
                "    # Clasificar\n",
                "    elif n_unique == 2 and set(df[col].dropna().unique()).issubset({0, 1, 0.0, 1.0}):\n",
                "        binary_cols.append(col)\n",
                "        discrete_cols.append(col)\n",
                "        info['category'] = 'binary'\n",
                "    elif n_unique <= 10:\n",
                "        categorical_cols.append(col)\n",
                "        discrete_cols.append(col)\n",
                "        info['category'] = 'categorical'\n",
                "        info['values'] = sorted(df[col].dropna().unique().tolist())\n",
                "    else:\n",
                "        continuous_cols.append(col)\n",
                "        info['category'] = 'continuous'\n",
                "        info['min'] = float(df[col].min())\n",
                "        info['max'] = float(df[col].max())\n",
                "        info['mean'] = float(df[col].mean())\n",
                "        info['std'] = float(df[col].std())\n",
                "    \n",
                "    column_info[col] = info\n",
                "\n",
                "print(f\"üìã CLASIFICACI√ìN DE COLUMNAS:\")\n",
                "print(f\"   Continuas: {len(continuous_cols)}\")\n",
                "print(f\"   Discretas: {len(discrete_cols)}\")\n",
                "print(f\"   - Binarias: {len(binary_cols)}\")\n",
                "print(f\"   - Categ√≥ricas (multi-clase): {len(categorical_cols)}\")\n",
                "print(f\"\\n‚ö†Ô∏è  PROBLEM√ÅTICAS (varianza cero): {len(zero_variance_cols)}\")\n",
                "for col in zero_variance_cols:\n",
                "    print(f\"   - {col}: solo valor = {df[col].unique()[0]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section_target",
            "metadata": {},
            "source": [
                "---\n",
                "## 2Ô∏è‚É£ An√°lisis del Target (Event/Duration)\n",
                "\n",
                "Per Getie Ayaneh (2020): La tasa de censura en estudios de empleabilidad t√≠picamente est√° entre 40-60%."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d9db5d25",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==============================================================================\n",
                "# AN√ÅLISIS DEL TARGET (Event/Duration)\n",
                "# ==============================================================================\n",
                "\n",
                "# Tasa de censura\n",
                "n_events = int(df['event'].sum())\n",
                "n_censored = len(df) - n_events\n",
                "censoring_rate = n_censored / len(df)\n",
                "\n",
                "target_info = {\n",
                "    'event': {\n",
                "        'n_events': n_events,\n",
                "        'n_censored': n_censored,\n",
                "        'censoring_rate': float(censoring_rate),\n",
                "        'event_rate': float(1 - censoring_rate)\n",
                "    },\n",
                "    'duration': {\n",
                "        'min': float(df['duration'].min()),\n",
                "        'max': float(df['duration'].max()),\n",
                "        'mean': float(df['duration'].mean()),\n",
                "        'median': float(df['duration'].median()),\n",
                "        'std': float(df['duration'].std()),\n",
                "        'n_unique': int(df['duration'].nunique())\n",
                "    }\n",
                "}\n",
                "\n",
                "print(f\"üéØ AN√ÅLISIS DEL TARGET:\")\n",
                "print(f\"   Eventos (E=1): {n_events} ({100*(1-censoring_rate):.1f}%)\")\n",
                "print(f\"   Censurados (E=0): {n_censored} ({100*censoring_rate:.1f}%)\")\n",
                "print(f\"\\n   Duration range: [{target_info['duration']['min']:.2f}, {target_info['duration']['max']:.2f}] meses\")\n",
                "print(f\"   Duration mean: {target_info['duration']['mean']:.2f} ¬± {target_info['duration']['std']:.2f}\")\n",
                "\n",
                "# Validaci√≥n cient√≠fica\n",
                "if 0.4 <= censoring_rate <= 0.6:\n",
                "    print(f\"\\n‚úÖ Tasa de censura ({censoring_rate:.1%}) dentro del rango esperado (40-60%)\")\n",
                "else:\n",
                "    print(f\"\\n‚ö†Ô∏è  Tasa de censura ({censoring_rate:.1%}) fuera del rango t√≠pico (40-60%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section_constraints",
            "metadata": {},
            "source": [
                "---\n",
                "## 3Ô∏è‚É£ Restricciones de Dominio\n",
                "\n",
                "Seg√∫n Lawless (2003), las restricciones fundamentales en survival data son:\n",
                "- `duration > 0` (el tiempo no puede ser negativo ni cero)\n",
                "- `event ‚àà {0, 1}` (indicador binario de censura)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c2d045bf",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==============================================================================\n",
                "# DETECCI√ìN DE RESTRICCIONES DE DOMINIO\n",
                "# ==============================================================================\n",
                "\n",
                "constraints = []\n",
                "\n",
                "# 1. Duration > 0 (CR√çTICO para survival)\n",
                "duration_positive = (df['duration'] > 0).all()\n",
                "constraints.append({\n",
                "    'variable': 'duration',\n",
                "    'constraint': 'duration > 0',\n",
                "    'satisfied': bool(duration_positive),\n",
                "    'violations': int((df['duration'] <= 0).sum()),\n",
                "    'criticality': 'HARD',\n",
                "    'reference': 'Lawless (2003): Time must be strictly positive'\n",
                "})\n",
                "\n",
                "# 2. Event binario\n",
                "event_binary = set(df['event'].unique()).issubset({0, 1})\n",
                "constraints.append({\n",
                "    'variable': 'event',\n",
                "    'constraint': 'event ‚àà {0, 1}',\n",
                "    'satisfied': bool(event_binary),\n",
                "    'violations': 0 if event_binary else 1,\n",
                "    'criticality': 'HARD',\n",
                "    'reference': 'Standard censoring indicator'\n",
                "})\n",
                "\n",
                "# 3. Edad razonable (basado en datos observados)\n",
                "if 'edad' in df.columns:\n",
                "    edad_min, edad_max = df['edad'].min(), df['edad'].max()\n",
                "    edad_valid = (df['edad'] >= 18) & (df['edad'] <= 65)\n",
                "    constraints.append({\n",
                "        'variable': 'edad',\n",
                "        'constraint': f'{int(edad_min)} ‚â§ edad ‚â§ {int(edad_max)}',\n",
                "        'satisfied': bool(edad_valid.all()),\n",
                "        'violations': int((~edad_valid).sum()),\n",
                "        'range_observed': [int(edad_min), int(edad_max)],\n",
                "        'criticality': 'SOFT',\n",
                "        'reference': 'Rango observado en la encuesta EPN'\n",
                "    })\n",
                "\n",
                "# 4. Soft skills normalizadas [0, 1]\n",
                "hab_cols = [c for c in df.columns if c.startswith('hab_')]\n",
                "for h in hab_cols:\n",
                "    in_range = (df[h] >= 0) & (df[h] <= 1)\n",
                "    constraints.append({\n",
                "        'variable': h,\n",
                "        'constraint': '0 ‚â§ value ‚â§ 1',\n",
                "        'satisfied': bool(in_range.all()),\n",
                "        'violations': int((~in_range).sum()),\n",
                "        'criticality': 'SOFT'\n",
                "    })\n",
                "\n",
                "# 5. Tech skills binarias\n",
                "tech_cols = [c for c in df.columns if c.startswith('tech_') and c not in zero_variance_cols]\n",
                "tech_constraint = {\n",
                "    'variable': 'tech_* (binarias)',\n",
                "    'constraint': 'value ‚àà {0, 1}',\n",
                "    'satisfied': True,\n",
                "    'violations': 0,\n",
                "    'n_features': len(tech_cols),\n",
                "    'criticality': 'HARD'\n",
                "}\n",
                "for t in tech_cols:\n",
                "    if not set(df[t].unique()).issubset({0, 1}):\n",
                "        tech_constraint['satisfied'] = False\n",
                "        tech_constraint['violations'] += 1\n",
                "constraints.append(tech_constraint)\n",
                "\n",
                "print(f\"üîí RESTRICCIONES DETECTADAS: {len(constraints)}\")\n",
                "for c in constraints:\n",
                "    status = \"‚úÖ\" if c['satisfied'] else \"‚ùå\"\n",
                "    crit = f\"[{c.get('criticality', 'SOFT')}]\" \n",
                "    print(f\"   {status} {crit} {c['constraint']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section_correlation",
            "metadata": {},
            "source": [
                "---\n",
                "## 4Ô∏è‚É£ An√°lisis de Correlaciones (NUEVO)\n",
                "\n",
                "Seg√∫n Andonovikj et al. (2024): Los sintetizadores deben preservar la correlaci√≥n duration-event. Analizamos correlaciones con el target para entender qu√© variables son predictivas."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "correlation_analysis",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==============================================================================\n",
                "# AN√ÅLISIS DE CORRELACIONES\n",
                "# ==============================================================================\n",
                "\n",
                "# Correlaci√≥n duration-event\n",
                "duration_event_corr = df['duration'].corr(df['event'])\n",
                "\n",
                "# Correlaciones con event (excluir duration y variables zero-variance)\n",
                "feature_cols = [c for c in df.columns if c not in ['event', 'duration'] + zero_variance_cols]\n",
                "correlations_with_event = {}\n",
                "correlations_with_duration = {}\n",
                "\n",
                "for col in feature_cols:\n",
                "    try:\n",
                "        correlations_with_event[col] = float(df[col].corr(df['event']))\n",
                "        correlations_with_duration[col] = float(df[col].corr(df['duration']))\n",
                "    except:\n",
                "        pass\n",
                "\n",
                "# Top correlaciones con event\n",
                "sorted_by_event = sorted(correlations_with_event.items(), key=lambda x: abs(x[1]), reverse=True)\n",
                "max_corr_event = sorted_by_event[0][1] if sorted_by_event else 0\n",
                "\n",
                "correlation_analysis = {\n",
                "    'duration_event_correlation': float(duration_event_corr),\n",
                "    'max_feature_event_correlation': float(max_corr_event),\n",
                "    'top_5_correlated_with_event': dict(sorted_by_event[:5]),\n",
                "    'warning': None\n",
                "}\n",
                "\n",
                "print(f\"üìà AN√ÅLISIS DE CORRELACIONES:\")\n",
                "print(f\"\\n   Duration ‚Üî Event: {duration_event_corr:.3f}\")\n",
                "if abs(duration_event_corr) > 0.3:\n",
                "    print(f\"   ‚ö†Ô∏è  Correlaci√≥n moderada: sintetizador debe preservar esta relaci√≥n\")\n",
                "    correlation_analysis['warning'] = 'High duration-event correlation must be preserved'\n",
                "\n",
                "print(f\"\\n   M√°xima correlaci√≥n feature‚Üíevent: {max_corr_event:.3f}\")\n",
                "if abs(max_corr_event) < 0.2:\n",
                "    print(f\"   üö® CR√çTICO: Ninguna feature tiene |corr| > 0.2 con event\")\n",
                "    print(f\"      ‚Üí Esto explica el bajo C-index (0.5669) del modelo\")\n",
                "\n",
                "print(f\"\\n   Top 5 correlaciones con event:\")\n",
                "for col, corr in sorted_by_event[:5]:\n",
                "    print(f\"      {col}: {corr:.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section_sparsity",
            "metadata": {},
            "source": [
                "---\n",
                "## 5Ô∏è‚É£ An√°lisis de Sparsity (Tech Skills)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "sparsity_analysis",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==============================================================================\n",
                "# AN√ÅLISIS DE SPARSITY EN TECH SKILLS\n",
                "# ==============================================================================\n",
                "\n",
                "tech_cols_all = [c for c in df.columns if c.startswith('tech_')]\n",
                "tech_sparsity = {}\n",
                "\n",
                "for col in tech_cols_all:\n",
                "    ones_pct = df[col].mean() * 100\n",
                "    tech_sparsity[col] = {\n",
                "        'ones_pct': float(ones_pct),\n",
                "        'ones_count': int(df[col].sum()),\n",
                "        'is_sparse': ones_pct < 5,  # <5% se considera sparse\n",
                "        'is_zero_variance': col in zero_variance_cols\n",
                "    }\n",
                "\n",
                "sparse_features = [k for k, v in tech_sparsity.items() if v['is_sparse'] and not v['is_zero_variance']]\n",
                "\n",
                "print(f\"üìä SPARSITY DE TECH SKILLS ({len(tech_cols_all)} features):\")\n",
                "print(f\"\\n   Zero-variance (excluir): {len(zero_variance_cols)}\")\n",
                "print(f\"   Sparse (<5% valores=1): {len(sparse_features)}\")\n",
                "print(f\"   Normales (‚â•5% valores=1): {len(tech_cols_all) - len(sparse_features) - len(zero_variance_cols)}\")\n",
                "\n",
                "if zero_variance_cols:\n",
                "    print(f\"\\n   ‚ùå Features con varianza cero (EXCLUIR del sintetizador):\")\n",
                "    for col in zero_variance_cols:\n",
                "        print(f\"      - {col}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section_cardinality",
            "metadata": {},
            "source": [
                "---\n",
                "## 6Ô∏è‚É£ Cardinalidad de Variables Categ√≥ricas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "880be100",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==============================================================================\n",
                "# CARDINALIDAD DE VARIABLES CATEG√ìRICAS\n",
                "# ==============================================================================\n",
                "\n",
                "cardinality = {}\n",
                "for col in categorical_cols + binary_cols:\n",
                "    if col not in zero_variance_cols:\n",
                "        cardinality[col] = {\n",
                "            'n_unique': int(df[col].nunique()),\n",
                "            'values': sorted([str(v) for v in df[col].unique().tolist()]),\n",
                "            'is_binary': col in binary_cols\n",
                "        }\n",
                "\n",
                "print(f\"üìä CARDINALIDAD (excluyendo zero-variance):\")\n",
                "for col in list(cardinality.keys())[:10]:\n",
                "    info = cardinality[col]\n",
                "    print(f\"   {col}: {info['n_unique']} valores {'(Binaria)' if info['is_binary'] else ''}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section_synthesis_rules",
            "metadata": {},
            "source": [
                "---\n",
                "## 7Ô∏è‚É£ Reglas del Juego para Generaci√≥n Sint√©tica (NUEVO)\n",
                "\n",
                "Basado en el an√°lisis anterior, definimos las reglas que debe seguir el sintetizador."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "synthesis_rules",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==============================================================================\n",
                "# REGLAS PARA GENERACI√ìN SINT√âTICA\n",
                "# ==============================================================================\n",
                "\n",
                "synthesis_rules = {\n",
                "    'hard_constraints': [\n",
                "        {\n",
                "            'rule': 'duration > 0',\n",
                "            'type': 'inequality',\n",
                "            'action': 'reject_if_violated',\n",
                "            'reference': 'Lawless (2003)'\n",
                "        },\n",
                "        {\n",
                "            'rule': 'event ‚àà {0, 1}',\n",
                "            'type': 'categorical',\n",
                "            'action': 'round_to_nearest'\n",
                "        },\n",
                "        {\n",
                "            'rule': 'tech_* ‚àà {0, 1}',\n",
                "            'type': 'binary',\n",
                "            'action': 'round_to_nearest'\n",
                "        }\n",
                "    ],\n",
                "    'soft_constraints': [\n",
                "        {\n",
                "            'rule': f'edad ‚àà [{int(df[\"edad\"].min())}, {int(df[\"edad\"].max())}]',\n",
                "            'type': 'range',\n",
                "            'action': 'clip_to_range'\n",
                "        },\n",
                "        {\n",
                "            'rule': 'hab_* ‚àà [0, 1]',\n",
                "            'type': 'range',\n",
                "            'action': 'clip_to_range'\n",
                "        },\n",
                "        {\n",
                "            'rule': f'duration ‚àà [{df[\"duration\"].min():.2f}, {df[\"duration\"].max():.2f}]',\n",
                "            'type': 'range',\n",
                "            'action': 'clip_to_range'\n",
                "        }\n",
                "    ],\n",
                "    'features_to_exclude': zero_variance_cols,\n",
                "    'preservation_targets': [\n",
                "        {\n",
                "            'metric': 'censoring_rate',\n",
                "            'target_value': float(censoring_rate),\n",
                "            'tolerance': 0.05\n",
                "        },\n",
                "        {\n",
                "            'metric': 'duration_event_correlation',\n",
                "            'target_value': float(duration_event_corr),\n",
                "            'tolerance': 0.1\n",
                "        }\n",
                "    ],\n",
                "    'recommended_synthesizer': {\n",
                "        'method': 'GaussianCopula',\n",
                "        'reason': f'N={len(df)} < 500, GANs inestables para small data (Xu et al., 2019)',\n",
                "        'alternative': 'CTGAN con epochs=300+ si se prefiere deep learning'\n",
                "    }\n",
                "}\n",
                "\n",
                "print(\"üéÆ REGLAS DEL JUEGO PARA SINTETIZADOR:\")\n",
                "print(f\"\\n   HARD CONSTRAINTS (no violar nunca):\")\n",
                "for r in synthesis_rules['hard_constraints']:\n",
                "    print(f\"   - {r['rule']}\")\n",
                "\n",
                "print(f\"\\n   SOFT CONSTRAINTS (preferible mantener):\")\n",
                "for r in synthesis_rules['soft_constraints']:\n",
                "    print(f\"   - {r['rule']}\")\n",
                "\n",
                "print(f\"\\n   EXCLUIR DEL SINTETIZADOR ({len(zero_variance_cols)} features):\")\n",
                "for col in zero_variance_cols:\n",
                "    print(f\"   - {col}\")\n",
                "\n",
                "print(f\"\\n   M√âTRICAS A PRESERVAR:\")\n",
                "for p in synthesis_rules['preservation_targets']:\n",
                "    print(f\"   - {p['metric']}: {p['target_value']:.3f} ¬± {p['tolerance']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "257a8525",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==============================================================================\n",
                "# GUARDAR REPORTE JSON MEJORADO\n",
                "# ==============================================================================\n",
                "\n",
                "# Referencias cient√≠ficas\n",
                "scientific_references = [\n",
                "    {\n",
                "        'author': 'Lawless, J.F.',\n",
                "        'year': 2003,\n",
                "        'title': 'Statistical Models and Methods for Lifetime Data',\n",
                "        'publisher': 'Wiley',\n",
                "        'relevance': 'Fundamentos de survival analysis, restricciones de dominio'\n",
                "    },\n",
                "    {\n",
                "        'author': 'Getie Ayaneh et al.',\n",
                "        'year': 2020,\n",
                "        'doi': '10.1155/2020/8653405',\n",
                "        'title': 'Survival Models for the Analysis of Waiting Time to First Employment',\n",
                "        'relevance': 'An√°lisis de tiempo-al-empleo en graduados'\n",
                "    },\n",
                "    {\n",
                "        'author': 'Andonovikj et al.',\n",
                "        'year': 2024,\n",
                "        'title': 'Survival analysis as semi-supervised multi-label classification',\n",
                "        'relevance': 'Relaci√≥n duration-event en s√≠ntesis de datos'\n",
                "    },\n",
                "    {\n",
                "        'author': 'Xu et al.',\n",
                "        'year': 2019,\n",
                "        'title': 'Modeling Tabular Data using Conditional GAN (CTGAN)',\n",
                "        'relevance': 'Sintetizadores para datos tabulares'\n",
                "    }\n",
                "]\n",
                "\n",
                "diagnosis = {\n",
                "    'metadata': {\n",
                "        'version': '2.0',\n",
                "        'created': '2026-01-08',\n",
                "        'author': 'Data Scientist Auditor',\n",
                "        'purpose': 'Diagn√≥stico para generaci√≥n sint√©tica de datos de supervivencia'\n",
                "    },\n",
                "    'scientific_references': scientific_references,\n",
                "    'dataset_info': {\n",
                "        'n_rows': int(len(df)),\n",
                "        'n_cols': int(len(df.columns)),\n",
                "        'source': str(DATA_PATH),\n",
                "        'original_source': 'Encuesta reci√©n graduados - pregrado (EPN)'\n",
                "    },\n",
                "    'column_classification': {\n",
                "        'continuous': continuous_cols,\n",
                "        'discrete': discrete_cols,\n",
                "        'binary': binary_cols,\n",
                "        'categorical': categorical_cols,\n",
                "        'zero_variance': zero_variance_cols\n",
                "    },\n",
                "    'column_details': column_info,\n",
                "    'target_analysis': target_info,\n",
                "    'correlation_analysis': correlation_analysis,\n",
                "    'constraints': constraints,\n",
                "    'cardinality': cardinality,\n",
                "    'sparsity_analysis': {\n",
                "        'n_sparse_features': len(sparse_features),\n",
                "        'n_zero_variance': len(zero_variance_cols),\n",
                "        'sparse_features': sparse_features\n",
                "    },\n",
                "    'synthesis_rules': synthesis_rules,\n",
                "    'problems_identified': []\n",
                "}\n",
                "\n",
                "# Identificar problemas\n",
                "if abs(max_corr_event) < 0.2:\n",
                "    diagnosis['problems_identified'].append(\n",
                "        'üö® CR√çTICO: Ninguna feature tiene correlaci√≥n > 0.2 con el target'\n",
                "    )\n",
                "if len(df) < 500:\n",
                "    diagnosis['problems_identified'].append(\n",
                "        f'‚ö†Ô∏è Dataset peque√±o (N={len(df)} < 500)'\n",
                "    )\n",
                "if len(zero_variance_cols) > 0:\n",
                "    diagnosis['problems_identified'].append(\n",
                "        f'‚ö†Ô∏è {len(zero_variance_cols)} features con varianza cero a excluir'\n",
                "    )\n",
                "if len(df.columns) / len(df) > 0.15:\n",
                "    diagnosis['problems_identified'].append(\n",
                "        f'‚ö†Ô∏è Alta dimensionalidad relativa (p/N = {len(df.columns)/len(df):.2f})'\n",
                "    )\n",
                "\n",
                "with open(OUTPUT_PATH, 'w') as f:\n",
                "    json.dump(diagnosis, f, indent=2, ensure_ascii=False)\n",
                "\n",
                "print(f\"\\n\" + \"=\"*70)\n",
                "print(f\"‚úÖ REPORTE GUARDADO: {OUTPUT_PATH}\")\n",
                "print(f\"=\"*70)\n",
                "print(f\"\\nüìã RESUMEN EJECUTIVO:\")\n",
                "print(f\"   Filas: {diagnosis['dataset_info']['n_rows']}\")\n",
                "print(f\"   Columnas: {diagnosis['dataset_info']['n_cols']}\")\n",
                "print(f\"   Tasa de censura: {target_info['event']['censoring_rate']:.1%}\")\n",
                "print(f\"   Restricciones OK: {sum(1 for c in constraints if c['satisfied'])}/{len(constraints)}\")\n",
                "print(f\"   Features zero-variance: {len(zero_variance_cols)}\")\n",
                "print(f\"\\n   Problemas identificados:\")\n",
                "for p in diagnosis['problems_identified']:\n",
                "    print(f\"   {p}\")"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}